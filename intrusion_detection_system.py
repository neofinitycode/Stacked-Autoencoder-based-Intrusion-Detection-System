# -*- coding: utf-8 -*-
"""IntrusionDetectionSystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12sZmdOMhuzQMcxA5VXk1tgu5sYN-YfWI
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_recall_curve
from sklearn.metrics import recall_score, classification_report, auc, roc_curve
from sklearn.metrics import precision_recall_fscore_support, f1_score
from sklearn.preprocessing import OneHotEncoder,LabelEncoder

import tensorflow as tf
from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras import regularizers
from keras.layers import BatchNormalization

import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('kddcup99_csv.csv')

#Check if any null values are exisiting in the data, also getting the number of categorical as well as numerical columns 
df.info()

#Checking for duplicate rows
duplicate_rows=df[df.duplicated()]
print(duplicate_rows)
df.drop_duplicates(keep=False,inplace=True)

#Protocol_type, Flag, Urgent were the categorical columns
print(df['protocol_type'].unique())
print(df['flag'].unique())
print(df['urgent'].unique())

df_copy=df

onehotencode=OneHotEncoder()
labelencode=LabelEncoder()

#Creating the labelencode for label class 0:- attack, 1:- normal
df_copy.loc[df_copy['label']!='normal','label']='attack'
df_copy['label']=labelencode.fit_transform(df_copy['label'])


#Creating OnehotEncoding for the categorical columns
X_protocol_type = onehotencode.fit_transform(df_copy.protocol_type.values.reshape(-1,1)).toarray()
X_service = onehotencode.fit_transform(df_copy.service.values.reshape(-1,1)).toarray()
X_flag = onehotencode.fit_transform(df_copy.flag.values.reshape(-1,1)).toarray()
 
#Creating dataframe for the OnehotEncoded columns 
df_protocol = pd.DataFrame(X_protocol_type, columns =df_copy['protocol_type'].unique()) 
df_service = pd.DataFrame(X_service, columns = df_copy['service'].unique()) 
df_flag = pd.DataFrame(X_flag, columns = df_copy['flag'].unique()) 



df_onehot_encoded=df_copy
lst_proto=df_protocol.columns
lst_service=df_service.columns
lst_flag=df_flag.columns


df_onehot_encoded= df_onehot_encoded.drop(['protocol_type','flag','service'], axis=1)

#Assigning remaining numerical columns to a single dataframe
for i in range(0,len(lst_proto)):
  df_onehot_encoded[lst_proto[i]]=df_protocol[lst_proto[i]].values
print(df_onehot_encoded.isna().sum().sum())

for i in range(0,len(lst_service)):
  df_onehot_encoded[lst_service[i]]=df_service[lst_service[i]].values

for i in range(0,len(lst_flag)):
  df_onehot_encoded[lst_flag[i]]=df_flag[lst_flag[i]].values

#Test Set is 20% of the total data
#Cross Validation set is 20% of the training data 

data_split_pct=0.2
#Test set
df_train_onehot_encoded,df_test_onehot_encoded=train_test_split(df_onehot_encoded,test_size=data_split_pct)
#cross validation set
df_train_onehot_encoded,df_cv_onehot_encoded=train_test_split(df_train_onehot_encoded,test_size=data_split_pct)

#Training the data on single class
df_train_onehot_encoded_normal=df_train_onehot_encoded.loc[df_train_onehot_encoded['label']!=0]
df_train_onehot_encoded_normal_wo_label=df_train_onehot_encoded_normal.drop(['label'],axis=1)
df_test_onehot_encoded_wo_label=df_test_onehot_encoded.drop(['label'],axis=1)
df_cv_onehot_encoded_wo_label=df_cv_onehot_encoded.drop(['label'],axis=1)

#Standardizing the values 
scaler = StandardScaler().fit(df_train_onehot_encoded_normal_wo_label)
df_train_sel_col_wo_label_rescaled = scaler.transform(df_train_onehot_encoded_normal_wo_label)
df_test_sel_col_wo_label_rescaled = scaler.transform(df_test_onehot_encoded_wo_label)
df_cv_sel_col_wo_label_rescaled = scaler.transform(df_cv_onehot_encoded_wo_label)

#ONE CLASS CLASSIFICAITION
#INITIALLY APPLYING AUTOENCODER
#DEFINING AUTOENCODERS PARAMETER
from keras.layers import BatchNormalization


epoch=500
batch_size=2000
input_dim=df_train_sel_col_wo_label_rescaled.shape[1]
encoding_dim=64
hidden_dim=int(encoding_dim/2)
learning_rate=1e-3


input_layer=Input(shape=(input_dim,))
encoder = Dense(encoding_dim, activation="relu", activity_regularizer=regularizers.l1(learning_rate))(input_layer)
encoder=BatchNormalization()(encoder)
encoder = Dense(hidden_dim, activation="relu")(encoder)
encoder=BatchNormalization()(encoder)
#decoder = Dense(hidden_dim, activation="relu")(encoder)
#decoder=BatchNormalization()(decoder)
decoder = Dense(encoding_dim, activation="relu")(encoder)#decoder
decoder=BatchNormalization()(decoder)
decoder = Dense(input_dim, activation="linear")(decoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)
#autoencoder.set_weights(tf.keras.initializers.he_normal())
initial_weights=autoencoder.get_weights()
autoencoder.summary()

autoencoder.compile(metrics=['mse'],
                    loss='mean_squared_error',
                    optimizer='adam')
cp = ModelCheckpoint(filepath="/content/drive/My Drive/IDS/autoencoder_classifier_one_hot_encoded_cv_v2.h5",
                               save_best_only=True,
                               verbose=0)
tb = TensorBoard(log_dir='./logs',
                histogram_freq=0,
                write_graph=True,
                write_images=True)
history = autoencoder.fit(df_train_sel_col_wo_label_rescaled, df_train_sel_col_wo_label_rescaled,
                    epochs=epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(df_cv_sel_col_wo_label_rescaled, df_cv_sel_col_wo_label_rescaled),
                    verbose=1,
                    callbacks=[cp, tb]).history

#EVALUATING CROSS VALIDATION SET
valid_x_predictions = autoencoder.predict(df_cv_sel_col_wo_label_rescaled)

#Calculating mean squared error between predictions and cros vaildation values
mse = np.mean(np.power(df_cv_sel_col_wo_label_rescaled - valid_x_predictions, 2), axis=1)
error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_cv_onehot_encoded['label']})
error_df = error_df.reset_index()

#Calculating precision and recall value at various threshold values
precision=[]
recall=[]
threshold_cv=[]
i=0
while(i<5):
  threshold_cv.append(i)
  pred_y_cv = [0 if e > i else 1 for e in error_df.Reconstruction_error.values]
  conf_matrix_cv = confusion_matrix(error_df.True_class, pred_y_cv)
  #print(conf_matrix_cv)
  print('threshold = ',i)
  print('Accuracy = ', round((conf_matrix_cv[0][0]+conf_matrix_cv[1][1])/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0]+conf_matrix_cv[1][0]+conf_matrix_cv[1][1])*100,2))
  Recall_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0])*100,2)
  Precision_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[1][0]+conf_matrix_cv[0][0])*100,2)
  precision.append(Precision_cv)
  recall.append(Recall_cv)
  i+=0.1

#Plotting graph between recall and precision based on various thresholds
plt.plot(threshold_cv, precision, label="Precision",linewidth=5)
plt.plot(threshold_cv, recall, label="Recall",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
#plt.savefig('recall&precisionVSthreshold.png')
plt.show()

#Selecting threshold based on the graph
best_threshold=1.1
pred_y_test = [0 if e > best_threshold else 1 for e in error_df_test.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df_test.True_class, pred_y_test)
print(conf_matrix)

print('Accuracy = ', round((conf_matrix[0][0]+conf_matrix[1][1])/(conf_matrix[0][1]+conf_matrix[0][0]+conf_matrix[1][0]+conf_matrix[1][1])*100,2))
Recall = round(conf_matrix[0][0]/(conf_matrix[0][1]+conf_matrix[0][0])*100,2)
Precision = round(conf_matrix[0][0]/(conf_matrix[1][0]+conf_matrix[0][0])*100,2)
print('Recall = ', Recall)
print('Precision = ', Precision)
print('F1-Score = ',round((2*Precision*Recall)/(Precision+Recall),2))
print('************************')

Labels = ["Attack","Normal"]
plt.figure(figsize=(3, 3))
sns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, a nnot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.savefig('confusion_matrix.png')
plt.show()


df=pd.read_csv('/content/drive/My Drive/IDS/kddcup99_csv.csv')
df.drop_duplicates(keep=False,inplace=True)

print(len(df.columns))
print(df.shape)
#print(len(df.columns))

print(df['protocol_type'].unique())
print(df['flag'].unique())
print(df['urgent'].unique())

df_copy=df

from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.feature_selection import chi2
onehotencode=OneHotEncoder()
labelencode=LabelEncoder()
#getting training and testing data

df_copy.loc[df_copy['label']!='normal','label']='attack'
df_copy['label']=labelencode.fit_transform(df_copy['label'])

X_protocol_type = onehotencode.fit_transform(df_copy.protocol_type.values.reshape(-1,1)).toarray()
X_service = onehotencode.fit_transform(df_copy.service.values.reshape(-1,1)).toarray()
X_flag = onehotencode.fit_transform(df_copy.flag.values.reshape(-1,1)).toarray()
 
df_protocol = pd.DataFrame(X_protocol_type, columns =df_copy['protocol_type'].unique()) 
df_service = pd.DataFrame(X_service, columns = df_copy['service'].unique()) 
df_flag = pd.DataFrame(X_flag, columns = df_copy['flag'].unique()) 



df_onehot_encoded=df_copy
lst_proto=df_protocol.columns
lst_service=df_service.columns
lst_flag=df_flag.columns
df_onehot_encoded= df_onehot_encoded.drop(['protocol_type','flag','service'], axis=1)

print(df_onehot_encoded.isna().sum().sum())

for i in range(0,len(lst_proto)):
  df_onehot_encoded[lst_proto[i]]=df_protocol[lst_proto[i]].values
print(df_onehot_encoded.isna().sum().sum())

for i in range(0,len(lst_service)):
  df_onehot_encoded[lst_service[i]]=df_service[lst_service[i]].values
print(df_onehot_encoded.isna().sum().sum())

for i in range(0,len(lst_flag)):
  df_onehot_encoded[lst_flag[i]]=df_flag[lst_flag[i]].values



data_split_pct=0.2
df_train_onehot_encoded,df_test_onehot_encoded=train_test_split(df_onehot_encoded,test_size=data_split_pct)
#cross validation set
df_train_onehot_encoded,df_cv_onehot_encoded=train_test_split(df_train_onehot_encoded,test_size=data_split_pct)

df_train_onehot_encoded.groupby(['label']).agg(['count'])['duration']

df_train_onehot_encoded_normal=df_train_onehot_encoded.loc[df_train_onehot_encoded['label']!=0]
df_train_onehot_encoded_normal_wo_label=df_train_onehot_encoded_normal.drop(['label'],axis=1)
df_test_onehot_encoded_wo_label=df_test_onehot_encoded.drop(['label'],axis=1)
df_cv_onehot_encoded_wo_label=df_cv_onehot_encoded.drop(['label'],axis=1)

#STANDARDIZING THE VALUES
scaler = StandardScaler().fit(df_train_onehot_encoded_normal_wo_label)
df_train_sel_col_wo_label_rescaled = scaler.transform(df_train_onehot_encoded_normal_wo_label)
df_test_sel_col_wo_label_rescaled = scaler.transform(df_test_onehot_encoded_wo_label)
df_cv_sel_col_wo_label_rescaled = scaler.transform(df_cv_onehot_encoded_wo_label)

#ONE CLASS CLASSIFICAITION
#INITIALLY APPLYING AUTOENCODER
#DEFINING AUTOENCODERS PARAMETER
#import keras
#import math
from keras.layers import BatchNormalization


epoch=500
batch_size=2000
input_dim=df_train_sel_col_wo_label_rescaled.shape[1]
encoding_dim=64
hidden_dim=int(encoding_dim/2)
learning_rate=1e-3

#initializer=keras.initializers.random_normal(stddev=1 / math.sqrt(self.feature_size))


input_layer=Input(shape=(input_dim,))
encoder = Dense(encoding_dim, activation="relu", activity_regularizer=regularizers.l1(learning_rate))(input_layer)
encoder=BatchNormalization()(encoder)
encoder = Dense(hidden_dim, activation="relu")(encoder)
encoder=BatchNormalization()(encoder)
#decoder = Dense(hidden_dim, activation="relu")(encoder)
#decoder=BatchNormalization()(decoder)
decoder = Dense(encoding_dim, activation="relu")(encoder)#decoder
decoder=BatchNormalization()(decoder)
decoder = Dense(input_dim, activation="linear")(decoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)
#autoencoder.set_weights(tf.keras.initializers.he_normal())
initial_weights=autoencoder.get_weights()
autoencoder.summary()


#TRAINING THE AUTOENCODER
autoencoder.compile(metrics=['mse'],
                    loss='mean_squared_error',
                    optimizer='adam')
cp = ModelCheckpoint(filepath="/content/drive/My Drive/IDS/autoencoder_classifier_one_hot_encoded_cv_v2.h5",
                               save_best_only=True,
                               verbose=0)
tb = TensorBoard(log_dir='./logs',
                histogram_freq=0,
                write_graph=True,
                write_images=True)
history = autoencoder.fit(df_train_sel_col_wo_label_rescaled, df_train_sel_col_wo_label_rescaled,
                    epochs=epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(df_cv_sel_col_wo_label_rescaled, df_cv_sel_col_wo_label_rescaled),
                    verbose=1,
                    callbacks=[cp, tb]).history

df_test_onehot_encoded.groupby(['label']).agg(['count'])['duration']

#GETTING GRADIENTS TO SEE WHETHER OVERCOME THE PROBLEM OF GRADIENT VANISHING
from keras import backend as k
outputtensor=autoencoder.output
listofvariables=autoencoder.trainable_weights
gradients=k.gradients(outputtensor,listofvariables)



layer=[]
i=0
k=0
while i<18:

  ll=[]
  print(initial_weights[k].shape,' ',listofvariables[i].shape)
  for j in range(0,initial_weights[k].shape[0]):
    
    ll.append(np.subtract(initial_weights[k][j],np.array(listofvariables[i][j])).mean())
    #ll.append()
  layer.append(ll)
  i+=4
  k+=6


#PLOTTING BOXPLOTS FOR GRADIENT VANISHING WITH AND WITHOUT BATCH NORMALIZATION
print(layer)
sns.boxplot(layer[5])

#EVALUATING CROSS VALIDATION SET
valid_x_predictions = autoencoder.predict(df_cv_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_cv_sel_col_wo_label_rescaled - valid_x_predictions, 2), axis=1)
error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_cv_onehot_encoded['label']})
error_df = error_df.reset_index()
precision=[]
recall=[]
threshold_cv=[]
for i in range(0,20):
  threshold_cv.append(i)
  pred_y_cv = [0 if e > i else 1 for e in error_df.Reconstruction_error.values]
  conf_matrix_cv = confusion_matrix(error_df.True_class, pred_y_cv)
  print(conf_matrix_cv)
  print('threshold = ',i)
  print('Accuracy = ', round((conf_matrix_cv[0][0]+conf_matrix_cv[1][1])/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0]+conf_matrix_cv[1][0]+conf_matrix_cv[1][1])*100,2))
  Recall_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[0][1]+conf_matrix_cv[0][0])*100,2)
  Precision_cv = round(conf_matrix_cv[0][0]/(conf_matrix_cv[1][0]+conf_matrix_cv[0][0])*100,2)
  precision.append(Precision_cv)
  recall.append(Recall_cv)

plt.plot(threshold_cv, precision, label="Precision",linewidth=3)
plt.plot(threshold_cv, recall, label="Recall",linewidth=3)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
#plt.xticks(np.arange(0, 10, step=1))
plt.ylabel('Precision/Recall')
plt.legend()
plt.savefig('/content/drive/My Drive/IDS/recall&precisionVSthreshold.png')
plt.show()

#EVALUATING TEST SET
test_x_predictions = autoencoder.predict(df_test_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_test_sel_col_wo_label_rescaled - test_x_predictions, 2), axis=1)
error_df_test = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_test_onehot_encoded['label']})

error_df_test = error_df_test.reset_index()
#threshold_fixed = 0.017

attack=error_df_test.loc[error_df_test['True_class']==0,'Reconstruction_error']
normal=error_df_test.loc[error_df_test['True_class']==1,'Reconstruction_error']

print('NORMAL')
print('MIN reconstruction_error = ', normal.min())
print('MEAN reconstruction_error = ', normal.mean())
print('MAX reconstruction_error = ', normal.max())
print('STD reconstruction_error = ', normal.std())
print('********************************')
print('ATTACK')
print('MIN reconstruction_error = ', attack.min())
print('MEAN reconstruction_error = ', attack.mean())
print('MAX reconstruction_error = ', attack.max())
print('STD reconstruction_error = ', attack.std())

#kwargs = dict(alpha=0.5, bins=100)
#plt.hist(normal, **kwargs, color='g', label='NORMAL')
#plt.hist(attack, **kwargs, color='r', label='ATTACK')
#plt.gca().set(title='Frequency Histogram of reconstruction error', ylabel='Frequency')
#plt.xlim(50,75)
#plt.legend();

#BESTUM BEST THRESHOLD=2

threshold_mera=1.1
pred_y_test = [0 if e > threshold_mera else 1 for e in error_df_test.Reconstruction_error.values]
conf_matrix_mera = confusion_matrix(error_df_test.True_class, pred_y_test)
print(conf_matrix_mera)
print('threshold = ',i)
print('Accuracy = ', round((conf_matrix_mera[0][0]+conf_matrix_mera[1][1])/(conf_matrix_mera[0][1]+conf_matrix_mera[0][0]+conf_matrix_mera[1][0]+conf_matrix_mera[1][1])*100,2))
Recall = round(conf_matrix_mera[0][0]/(conf_matrix_mera[0][1]+conf_matrix_mera[0][0])*100,2)
Precision = round(conf_matrix_mera[0][0]/(conf_matrix_mera[1][0]+conf_matrix_mera[0][0])*100,2)
print('Recall = ', Recall)
print('Precision = ', Precision)
print('F1-Score = ',round((2*Precision*Recall)/(Precision+Recall),2))
print('************************')

LABELS = ["Attack","Normal"]
#pred_y_test = [0 if e > threshold_fixed else 1 for e in error_df_test.Reconstruction_error.values]
#conf_matrix = confusion_matrix_mera(error_df_test.True_class, pred_y_test)
plt.figure(figsize=(3, 3))
sns.heatmap(conf_matrix_mera, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.savefig('/content/drive/My Drive/IDS/confusion_matrix.png')
plt.show()

autoencoder.save('/content/drive/My Drive/IDS/autoencoder_v2.h5')



#ONE CLASS CLASSIFICAITION
#INITIALLY APPLYING AUTOENCODER
#DEFINING AUTOENCODERS PARAMETER
#import keras
#import math
from keras.layers import BatchNormalization


epoch=500
batch_size=2000
input_dim=df_train_sel_col_wo_label_rescaled.shape[1]
encoding_dim=64
hidden_dim=int(encoding_dim/2)
learning_rate=1e-3

#initializer=keras.initializers.random_normal(stddev=1 / math.sqrt(self.feature_size))


input_layer1=Input(shape=(input_dim,))
encoder1 = Dense(encoding_dim, activation="relu", activity_regularizer=regularizers.l1(learning_rate))(input_layer1)
#encoder=BatchNormalization()(encoder)
encoder1 = Dense(hidden_dim, activation="relu")(encoder1)
#encoder=BatchNormalization()(encoder)
decoder1 = Dense(hidden_dim, activation="relu")(encoder1)
#decoder=BatchNormalization()(decoder)
decoder1 = Dense(encoding_dim, activation="relu")(decoder1)
#decoder=BatchNormalization()(decoder)
decoder1 = Dense(input_dim, activation="linear")(decoder1)
autoencoder1 = Model(inputs=input_layer1, outputs=decoder1)
#autoencoder.set_weights(tf.keras.initializers.he_normal())
#initial_weights=autoencoder.get_weights()
autoencoder1.summary()

#TRAINING THE AUTOENCODER
#TRAINING THE AUTOENCODER
autoencoder1.compile(metrics=['mse'],
                    loss='mean_squared_error',
                    optimizer='adam')
cp = ModelCheckpoint(filepath="/content/drive/My Drive/IDS/autoencoder_classifier_one_hot_encoded_cv_wo_BN.h5",
                               save_best_only=True,
                               verbose=0)
tb = TensorBoard(log_dir='./logs',
                histogram_freq=0,
                write_graph=True,
                write_images=True)
history = autoencoder1.fit(df_train_sel_col_wo_label_rescaled, df_train_sel_col_wo_label_rescaled,
                    epochs=epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(df_cv_sel_col_wo_label_rescaled, df_cv_sel_col_wo_label_rescaled),
                    verbose=1,
                    callbacks=[cp, tb]).history

#GETTING GRADIENTS TO SEE WHETHER OVERCOME THE PROBLEM OF GRADIENT VANISHING
from keras import backend as k
outputtensor_wo_BN=autoencoder1.output
listofvariables_wo_BN=autoencoder1.trainable_weights
gradients_wo_BN=k.gradients(outputtensor_wo_BN,listofvariables_wo_BN)

layer_wo_BN=[]
i=0
k=0
while i<10:

  ll=[]
  print(initial_weights[k].shape,' ',listofvariables_wo_BN[i].shape)
  for j in range(0,initial_weights[k].shape[0]):
    
    ll.append(np.subtract(initial_weights[k][j],np.array(listofvariables_wo_BN[i][j])).mean())
    #ll.append()
  layer_wo_BN.append(ll)
  i+=2
  k+=6

layer_1=[]
layer_1.append(layer_wo_BN[0])
layer_1.append(layer[0])

layer_2=[layer_wo_BN[1],layer[1]]
layer_3=[layer_wo_BN[2],layer[2]]
layer_4=[layer_wo_BN[3],layer[3]]
layer_5=[layer_wo_BN[4],layer[4]]

sns.boxplot(data=layer_1)

sns.boxplot(data=layer_2)

sns.boxplot(data=layer_3)

sns.boxplot(data=layer_4)

sns.boxplot(data=layer_5)



#ONE CLASS CLASSIFIER
#TRYING ONLY LSTM 
from keras.layers import LSTM,Dropout,Activation
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from keras.models import Sequential


X_train = np.reshape(df_train_sel_col_wo_label_rescaled, (df_train_sel_col_wo_label_rescaled.shape[0], 1, df_train_sel_col_wo_label_rescaled.shape[1]))
X_test = np.reshape(df_test_sel_col_wo_label_rescaled, (df_test_sel_col_wo_label_rescaled.shape[0], 1, df_test_sel_col_wo_label_rescaled.shape[1]))

y_train = np.array(df_train_onehot_encoded_normal['label'])
y_test = np.array(df_test_onehot_encoded['label'])



model = Sequential()
model.add(LSTM(3,input_dim=df_train_sel_col_wo_label_rescaled.shape[1]))  # try using a GRU instead, for fun
model.add(Dropout(0.1))
model.add(Dense(1))
model.add(Activation('linear'))
print(model.get_config())


# try using different optimizers and different optimizer configs
model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['mse'])
#checkpointer = callbacks.ModelCheckpoint(filepath="kddresults_lstm_only/lstm1layer/checkpoint-{epoch:02d}.hdf5", verbose=1, save_best_only=True, monitor='val_acc',mode='max')
#csv_logger = CSVLogger('training_set_iranalysis.csv',separator=',', append=False)
model.fit(X_train, y_train, batch_size=2000, nb_epoch=500)
#model.save("kddresults/lstm1layer/fullmodel/lstm1layer_model.hdf5")



#EVALUATING TEST SET
test_x_predictions_lstm = model.predict(X_test)
mse_lstm = np.mean(np.power(df_test_sel_col_wo_label_rescaled - test_x_predictions_lstm, 2), axis=1)
error_df_test_lstm = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_test_onehot_encoded['label']})

error_df_test_lstm = error_df_test_lstm.reset_index()
#threshold_fixed = 0.017

attack=error_df_test_lstm.loc[error_df_test['True_class']==0,'Reconstruction_error']
normal=error_df_test_lstm.loc[error_df_test['True_class']==1,'Reconstruction_error']

for i in range(0,10):
  threshold_mera_lstm=i
  pred_y_test_lstm = [0 if e > threshold_mera_lstm else 1 for e in error_df_test_lstm.Reconstruction_error.values]
  conf_matrix_mera_lstm = confusion_matrix(error_df_test_lstm.True_class, pred_y_test_lstm)
  print(conf_matrix_mera_lstm)
  print('threshold = ',i)
  print('Accuracy = ', round((conf_matrix_mera_lstm[0][0]+conf_matrix_mera_lstm[1][1])/(conf_matrix_mera_lstm[0][1]+conf_matrix_mera_lstm[0][0]+conf_matrix_mera_lstm[1][0]+conf_matrix_mera_lstm[1][1])*100,2))
  print('Recall = ', round(conf_matrix_mera_lstm[0][0]/(conf_matrix_mera_lstm[0][1]+conf_matrix_mera_lstm[0][0])*100,2))
  print('************************')

error_df_test_lstm.loc[error_df_test_lstm['True_class']==0].Reconstruction_error.max()

#retrieving misclassified labels
error_df_test['predicted']=pred_y_test
error_df_mis


#EVALUATING TEST SET
test_x_predictions = autoencoder.predict(df_test_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_test_sel_col_wo_label_rescaled - test_x_predictions, 2), axis=1)
error_df_test = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_test_onehot_encoded['label']})

error_df_test = error_df_test.reset_index()
threshold_fixed = 0.017
groups = error_df_test.groupby('True_class')


fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "normal" if name == 1 else "attack")
ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();

#DRAWING CONFUSION MATRIX
LABELS = ["Attack","Normal"]
pred_y_test = [0 if e > threshold_fixed else 1 for e in error_df_test.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df_test.True_class, pred_y_test)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()




#CHECK LABELS ARE CORRECTLY LABEL ENCODED
df_train_onehot_encoded.groupby(['label']).agg(['count'])['duration']

df_train_onehot_encoded_wo_label=df_train_onehot_encoded.drop('label',axis=1)
df_train_onehot_encoded_label=df_train_onehot_encoded['label']

#SELECTING IMPORTANT FEATURES
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
#Single decision tree
"""s_tree=SelectFromModel(DecisionTreeClassifier(criterion="entropy"))
s_tree.fit(df_train_wo_label,df_train_label)
print(s_tree.get_support())
print(df_train_wo_label.columns[s_tree.get_support()])
"""

#random forest
m_trees_onehot=SelectFromModel(RandomForestClassifier(n_estimators=200,criterion="entropy"))
m_trees_onehot.fit(df_train_onehot_encoded_wo_label,df_train_onehot_encoded_label)
#print(m_trees.get_support())

imp_feat_col=df_train_onehot_encoded_wo_label.columns[m_trees_onehot.get_support()]
print(imp_feat_col)



#1 Normal
#0 Attack

#SELECTING FEATURES GIVEN RANDOM FOREST
df_train_onehot_encoded_wo_label=df_train_onehot_encoded_wo_label[df_train_onehot_encoded_wo_label.columns[m_trees_onehot.get_support()]]
df_cv_onehot_encoded_wo_label=df_cv_onehot_encoded.drop(['label'],axis=1)
df_cv_onehot_encoded_wo_label=df_cv_onehot_encoded_wo_label[df_cv_onehot_encoded_wo_label.columns[m_trees_onehot.get_support()]]
df_test_onehot_encoded_wo_label=df_test_onehot_encoded.drop(['label'],axis=1)
df_test_onehot_encoded_wo_label=df_test_onehot_encoded_wo_label[df_test_onehot_encoded_wo_label.columns[m_trees_onehot.get_support()]]

#STANDARDIZING THE VALUES
scaler = StandardScaler().fit(df_train_onehot_encoded_wo_label)
df_train_sel_col_wo_label_rescaled = scaler.transform(df_train_onehot_encoded_wo_label)
df_cv_sel_col_wo_label_rescaled = scaler.transform(df_cv_onehot_encoded_wo_label)
df_test_sel_col_wo_label_rescaled = scaler.transform(df_test_onehot_encoded_wo_label)

#DEFINING AUTOENCODERS PARAMETER
epoch=500
batch_size=2000
input_dim=df_train_sel_col_wo_label_rescaled.shape[1]
encoding_dim=64
hidden_dim=int(encoding_dim/2)
learning_rate=1e-3

input_layer=Input(shape=(input_dim,))
encoder = Dense(encoding_dim, activation="relu", activity_regularizer=regularizers.l1(learning_rate))(input_layer)
encoder = Dense(hidden_dim, activation="relu")(encoder)
decoder = Dense(hidden_dim, activation="relu")(encoder)
decoder = Dense(encoding_dim, activation="relu")(decoder)
decoder = Dense(input_dim, activation="linear")(decoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.summary()

#TRAINING THE AUTOENCODER
autoencoder.compile(metrics=['accuracy'],
                    loss='mean_squared_error',
                    optimizer='adam')
cp = ModelCheckpoint(filepath="autoencoder_classifier_one_hot_encoded.h5",
                               save_best_only=True,
                               verbose=0)
tb = TensorBoard(log_dir='./logs',
                histogram_freq=0,
                write_graph=True,
                write_images=True)
history = autoencoder.fit(df_train_sel_col_wo_label_rescaled, df_train_sel_col_wo_label_rescaled,
                    epochs=epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(df_cv_sel_col_wo_label_rescaled, df_cv_sel_col_wo_label_rescaled),
                    verbose=1,
                    callbacks=[cp, tb]).history

#EVALUATING CROSS VALIDATION SET
valid_x_predictions = autoencoder.predict(df_cv_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_cv_sel_col_wo_label_rescaled - valid_x_predictions, 2), axis=1)
error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_cv_onehot_encoded['label']})
error_df = error_df.reset_index()
precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)
plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)
plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
plt.show()

#EVALUATING TEST SET
test_x_predictions = autoencoder.predict(df_test_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_test_sel_col_wo_label_rescaled - test_x_predictions, 2), axis=1)
error_df_test = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_test_onehot_encoded['label']})

error_df_test = error_df_test.reset_index()
threshold_fixed = 0.017
groups = error_df_test.groupby('True_class')


fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "normal" if name == 1 else "attack")
ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();

#DRAWING CONFUSION MATRIX
LABELS = ["Attack","Normal"]
pred_y_test = [0 if e > threshold_fixed else 1 for e in error_df_test.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df_test.True_class, pred_y_test)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

#EVALUATING TRAIN SET TO SEE HOW MANY INSTANCES OF ALL ARE MISCLASSIFIED
train_x_predictions = autoencoder.predict(df_train_sel_col_wo_label_rescaled)
mse = np.mean(np.power(df_train_sel_col_wo_label_rescaled - train_x_predictions, 2), axis=1)
error_df_train = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': df_train_onehot_encoded['label']})

error_df_train = error_df_train.reset_index()
threshold_fixed = 0.017
groups = error_df_train.groupby('True_class')


fig, ax = plt.subplots()
for name, group in groups:
    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "normal" if name == 1 else "attack")
ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();

#DRAWING CONFUSION MATRIX
LABELS = ["Attack","Normal"]
pred_y = [0 if e > threshold_fixed else 1 for e in error_df_train.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df_train.True_class, pred_y)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()



#retrieving misclassified training data
error_df_train['predicted']=pred_y
error_mis_df_train=error_df_train.loc[error_df_train['True_class']!=error_df_train['predicted']]
df_train_misclass_onehot_encoded_wo_label=df_train_onehot_encoded_wo_label.iloc[error_mis_df_train.index]
df_train_misclass_onehot_encoded=df_train_onehot_encoded.iloc[error_mis_df_train.index]

#retrieving misclassified cross validation data
pred_y_cv = [0 if e > threshold_fixed else 1 for e in error_df.Reconstruction_error.values]
error_df['predicted']=pred_y_cv
error_mis_df_cv=error_df.loc[error_df['True_class']!=error_df['predicted']]
print(error_mis_df_cv.shape)
df_cv_misclass_onehot_encoded_wo_label=df_cv_onehot_encoded_wo_label.iloc[error_mis_df_cv.index]
df_cv_misclass_onehot_encoded=df_cv_onehot_encoded.iloc[error_mis_df_cv.index]


#retrieving misclassified test data
error_df_test['predicted']=pred_y_test
error_mis_df_test=error_df_test.loc[error_df_test['True_class']!=error_df_test['predicted']]
df_test_misclass_onehot_encoded_wo_label=df_test_onehot_encoded_wo_label.iloc[error_mis_df_test.index]
df_test_misclass_onehot_encoded=df_test_onehot_encoded.iloc[error_mis_df_test.index]





error_mis_df_train.shape

#STANDARDIZING THE VALUES
scaler = StandardScaler().fit(df_train_misclass_onehot_encoded_wo_label)
df_train_misclass_sel_col_wo_label_rescaled = scaler.transform(df_train_misclass_onehot_encoded_wo_label)
df_cv_misclass_sel_col_wo_label_rescaled = scaler.transform(df_cv_misclass_onehot_encoded_wo_label)
df_test_misclass_sel_col_wo_label_rescaled = scaler.transform(df_test_misclass_onehot_encoded_wo_label)

#TRYING LSTM
from keras.layers import LSTM,Dropout,Activation
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from keras.models import Sequential


X_train = np.reshape(df_train_misclass_sel_col_wo_label_rescaled, (df_train_misclass_sel_col_wo_label_rescaled.shape[0], 1, df_train_misclass_sel_col_wo_label_rescaled.shape[1]))
X_test = np.reshape(df_cv_misclass_sel_col_wo_label_rescaled, (df_cv_misclass_sel_col_wo_label_rescaled.shape[0], 1, df_cv_misclass_sel_col_wo_label_rescaled.shape[1]))

y_train = np.array(df_train_misclass_onehot_encoded['label'])
y_test = np.array(df_cv_misclass_onehot_encoded['label'])



model = Sequential()
model.add(LSTM(4,input_dim=df_train_misclass_sel_col_wo_label_rescaled.shape[1]))  # try using a GRU instead, for fun
model.add(Dropout(0.1))
model.add(Dense(1))
model.add(Activation('sigmoid'))
print(model.get_config())


# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
checkpointer = callbacks.ModelCheckpoint(filepath="kddresults/lstm1layer/checkpoint-{epoch:02d}.hdf5", verbose=1, save_best_only=True, monitor='val_acc',mode='max')
csv_logger = CSVLogger('training_set_iranalysis.csv',separator=',', append=False)
model.fit(X_train, y_train, batch_size=500, nb_epoch=200, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])
#model.save("kddresults/lstm1layer/fullmodel/lstm1layer_model.hdf5")

loss, accuracy = model.evaluate(X_test, y_test)
print("\nLoss: %.2f, Accuracy: %.2f%%" % (loss, accuracy*100))
X_test_data = np.reshape(df_test_misclass_sel_col_wo_label_rescaled, (df_test_misclass_sel_col_wo_label_rescaled.shape[0], 1, df_test_misclass_sel_col_wo_label_rescaled.shape[1]))
y_test_data = np.array(df_test_misclass_onehot_encoded['label'])
y_pred = model.predict_classes(X_test_data)

LABELS = ["Attack","Normal"]
#pred_y = [0 if e > threshold_fixed else 1 for e in error_df_train.Reconstruction_error.values]
conf_matrix = confusion_matrix(y_test_data, y_pred)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()



#TRYING ONLY LSTM
from keras.layers import LSTM,Dropout,Activation
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from keras.models import Sequential


X_train = np.reshape(df_train_sel_col_wo_label_rescaled, (df_train_sel_col_wo_label_rescaled.shape[0], 1, df_train_sel_col_wo_label_rescaled.shape[1]))
X_test = np.reshape(df_cv_sel_col_wo_label_rescaled, (df_cv_sel_col_wo_label_rescaled.shape[0], 1, df_cv_sel_col_wo_label_rescaled.shape[1]))

y_train = np.array(df_train_onehot_encoded['label'])
y_test = np.array(df_cv_onehot_encoded['label'])



model = Sequential()
model.add(LSTM(4,input_dim=df_train_sel_col_wo_label_rescaled.shape[1]))  # try using a GRU instead, for fun
model.add(Dropout(0.1))
model.add(Dense(1))
model.add(Activation('sigmoid'))
print(model.get_config())


# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
checkpointer = callbacks.ModelCheckpoint(filepath="kddresults_lstm_only/lstm1layer/checkpoint-{epoch:02d}.hdf5", verbose=1, save_best_only=True, monitor='val_acc',mode='max')
csv_logger = CSVLogger('training_set_iranalysis.csv',separator=',', append=False)
model.fit(X_train, y_train, batch_size=500, nb_epoch=200, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])
#model.save("kddresults/lstm1layer/fullmodel/lstm1layer_model.hdf5")

loss, accuracy = model.evaluate(X_test, y_test)
print("\nLoss: %.2f, Accuracy: %.2f%%" % (loss, accuracy*100))
X_test_data = np.reshape(df_test_sel_col_wo_label_rescaled, (df_test_sel_col_wo_label_rescaled.shape[0], 1, df_test_sel_col_wo_label_rescaled.shape[1]))
y_test_data = np.array(df_test_onehot_encoded['label'])
y_pred = model.predict_classes(X_test_data)

LABELS = ["Attack","Normal"]
#pred_y = [0 if e > threshold_fixed else 1 for e in error_df_train.Reconstruction_error.values]
conf_matrix = confusion_matrix(y_test_data, y_pred)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()



#VARIATIONAL AUTOENCODER
from keras.layers import Lambda
from keras import backend as K
epoch=500
batch_size=2000
input_dim=df_train_sel_col_wo_label_rescaled.shape[1]
intermediate_dim=32
latent_dim=16

x = Input(batch_shape=(batch_size, input_dim))
h = Dense(intermediate_dim, activation='relu')(x)
z_mean = Dense(latent_dim)(h)
z_log_sigma = Dense(latent_dim)(h)

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(batch_size, latent_dim),
                              mean=0., std=epsilon_std)
    return z_mean + K.exp(z_log_sigma) * epsilon

# note that "output_shape" isn't necessary with the TensorFlow backend
# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`
z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])


decoder_h = Dense(intermediate_dim, activation='relu')
decoder_mean = Dense(input_dim, activation='linear')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)

#CREATING THE MODEL
vae = Model(x, x_decoded_mean)

# encoder, from inputs to latent space
encoder = Model(x, z_mean)

# generator, from latent space to reconstructed inputs
decoder_input = Input(shape=(latent_dim,))
_h_decoded = decoder_h(decoder_input)
_x_decoded_mean = decoder_mean(_h_decoded)
generator = Model(decoder_input, _x_decoded_mean)

##TRAINING VARIATIONAL AUTOENCODER

def vae_loss(x, x_decoded_mean):
    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)
    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)
    return xent_loss + kl_loss

vae.compile(optimizer='adam', loss=vae_loss)


vae.fit(x_train, x_train,
        shuffle=True,
        epochs=epoch,
        batch_size=batch_size,
        validation_data=(x_test, x_test))